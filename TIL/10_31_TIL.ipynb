{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6c05f11",
   "metadata": {},
   "source": [
    "오늘의 시작은 어제 정리한 LSTM모델의 정확도를 늘리는 방법에 대하여 알아보았습니다.\n",
    "\n",
    " \n",
    "\n",
    "정확도를 높이기 위해서 수정한 코드를 작동했는데 시간이 너무 오래걸렸습니다. (대략 5시간 정도).\n",
    "\n",
    " \n",
    "\n",
    "정확도는 크게 개선 되지 않았습니다.\n",
    "\n",
    " \n",
    "\n",
    "지금 현재로는 편향 되어있는 데이터셋을 사용해서 정확도의 개선이 안된 것이라 생각하고있습니다.\n",
    "\n",
    " \n",
    "\n",
    "모델학습을 기다리면서 오늘 새롭게 배부 된 AI활용 강의를 2주차까지 들었습니다.\n",
    "\n",
    " \n",
    "\n",
    "강의를 들으며 코드를 작성하다 패키지들에 오류가 생겨서 또 시간이 많이 들어가게 되었지만\n",
    "\n",
    " \n",
    "\n",
    "새로운 가상환경을 만들어 해결했고 가상환경에 대한 복습이 되는 시간이였습니다.\n",
    "\n",
    " \n",
    "\n",
    "API를 활용하고 오픈소스를 활용하는 부분은 알것같으면서도 모르겟습니다. \n",
    "\n",
    " \n",
    "\n",
    "정확히 어떻게 써야하고 무엇을 조심해야하는지 누가 옆에서 상상으로 말한듯한? 그런느낌입니다.\n",
    "\n",
    " \n",
    "\n",
    "강의를 따라가며 익숙해져야 할 것 같습니다.\n",
    "\n",
    "# ☑️ 도전 과제\n",
    "\n",
    "## LSTM 모델 성능 개선\n",
    "\n",
    "우선 LSTM모델의 성능이 아래와 같이 나왔다.\n",
    "```python\n",
    "Epoch 1, Validation Accuracy: 0.62\n",
    "Epoch 2, Validation Accuracy: 0.64\n",
    "Epoch 3, Validation Accuracy: 0.64\n",
    "Epoch 4, Validation Accuracy: 0.65\n",
    "Epoch 5, Validation Accuracy: 0.64\n",
    "Epoch 6, Validation Accuracy: 0.64\n",
    "Epoch 7, Validation Accuracy: 0.63\n",
    "Epoch 8, Validation Accuracy: 0.63\n",
    "Epoch 9, Validation Accuracy: 0.62\n",
    "Epoch 10, Validation Accuracy: 0.61\n",
    "```\n",
    "\n",
    "해석을 하면\n",
    "\n",
    "Epoch 1에서 정확도가 0.62에서 Epoch 4까지 0.65까지 증가하였지만 이후 조금씩 떨어지다 마지막엔 Epoch 1보다 더 떨어지는 정확도를 보여주었다.\n",
    "\n",
    "\n",
    "이럴 때는 과적합과 학습 후반부의 이상 검증 성능을 개선하지 못하고있다는 의심을 할 수 있다.\n",
    "\n",
    "\n",
    "과적합이란 모델이 학습 데이터에 너무 적합하게 훈련되어 새로운 데이터(검증 데이터)에 대한 성능이 떨어지는 현상이다.\n",
    "\n",
    " \n",
    "\n",
    "### 정확도 개선\n",
    "\n",
    "이때 개선 방법으로는 학습률, 모델구조 변경, 규제기법 적용이 필요하다.\n",
    "\n",
    "구두점 제거 전처리 과정을 모델코드의 앞부분에 넣고\n",
    "\n",
    "* embed_dim의 수치를 64 -> 100으로 증가\n",
    "\n",
    "* hidden_dim의 수치를 128에서 256으로 증가\n",
    "\n",
    "* 학습률을 조정할 스케줄러 추가\n",
    "> scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "* LSTM모델 정의부분에 num_layers=2 추가.\n",
    "\n",
    "> self.dropout = nn.Dropout(0.5) 드롭아웃을 설정\n",
    "\n",
    "* 옵티마지어 설정 매개변수로 weight_decay=0.0001로 설청 추가했습니다.\n",
    " \n",
    "\n",
    "또한 이렇게 진행했을 때 학습속도가 너무 느려 학습속도를 개선해주는 방법을 알아 보았다.\n",
    "\n",
    " \n",
    "\n",
    "### 학습속도 개선\n",
    "\n",
    "자연어 처리(NLP)작업에서 사건 순서에 따라 데이터를 처리하기 때문에 특히 길이가 긴 텍스트나 대용량 데이터셋을 다룰 때 학습 속도가 느려진다.\n",
    "\n",
    "이를 개선하기 위해 \n",
    "\n",
    "* batch_size를 늘려 한번에 처리되는 데이터양을 늘려주고\n",
    "\n",
    "* hidden_dim을 줄여 모데 파라미터 수를 줄여주어야하고\n",
    "\n",
    "* 양방향 LSTM( Bidirectional )을 제거\n",
    "\n",
    "* 이미 들어가있던 작업으로는 2가지가 더 있다.\n",
    "\n",
    " * GPU 사용: GPU가 설정되어 있다면 모델과 데이터를 cuda()를 통해 GPU로 이동시켜 처리 속도를 높일 수 있습니다.\n",
    " * 속도가 빠른 옵티마이저 사용: Adam이나 AdaGrad 같은 최적화 알고리즘은 SGD보다 수렴 속도가 빠를 수 있습니다.\n",
    " \n",
    "\n",
    "\n",
    "정확도 개선을 위해 추가한 작업들의 설명을 적어 보겟습니다.\n",
    "\n",
    " \n",
    "\n",
    "* embed_dim의 수치를 64 -> 100으로 증가\n",
    "임베딩 차원이 커지면 단어의 표현력이 높아져, 단어 간의 관계를 더 잘 포착할 수 있다.\n",
    "\n",
    " \n",
    "\n",
    "* hidden_dim의 수치를 128에서 256으로 증가\n",
    "히든 차원이 커지면 더 많은 정보를 저장할 수 있어 긴 문맥을 덩 잘 학습할 수 있다.\n",
    "\n",
    " \n",
    "\n",
    "* 학습률을 조정할 스케줄러 추가\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "이 코드를 추가하면 학습률이 step_size마다 gamma 비율로 감소해 학습 중에 ㅊ히적화가 가능하다.\n",
    "\n",
    " \n",
    "\n",
    "* LSTM모델 정의부분에 num_layers=2 추가.\n",
    "LSTM을 2개 이상으로 쌓아 더 깊은 모델을 구성해 볼 수 있다.\n",
    "\n",
    " \n",
    "\n",
    "* bidirectional=True를 설정하면, 양방향 LSTM을 적용\n",
    "텍스트를 앞에서 읽을 뿐만 아니라 뒤에서 읽을 수도 있게 하여 문맥 이해에 도움\n",
    "\n",
    " \n",
    "\n",
    "* self.dropout = nn.Dropout(0.5) 드롭아웃을 설정\n",
    "드롭아웃을 활용하여 오버피팅을 방지할 수 있다.\n",
    "\n",
    " \n",
    "\n",
    "* 옵티마지어 설정 매개변수로 weight_decay=0.0001로 설청 추가\n",
    "옵티마이저에 weight_decay를 설정해 가중치 감소 기법을 적용해 과적합을 방지할 수 있다.\n",
    "\n",
    "# ☑️ 회고\n",
    " \n",
    "\n",
    "* split메서드: 공백을 기준으로 단어별로 분할하여 리스트에 저장할 수 있게 해준다.\n",
    "\n",
    " \n",
    "\n",
    "* pop(index) :\n",
    "\n",
    " - 리스트의 특정 인덱스 위치에 있는 요소 제거하고 그 값을 반환\n",
    "\n",
    " - 인덱스를 지정하지 않으면 마지막 요소가 제거 된다.\n",
    "\n",
    " - 주로 특정 요소를 제거하거나 리스트를 조작할 때 유용\n",
    "\n",
    " \n",
    "\n",
    "* random.randit(a, b)\n",
    "\n",
    " - a와 b 사이의 정수 중 무작위 하나를 반환한다.\n",
    "\n",
    " - a와 b의 양쪽 경계가 포함된다. ex) randit(1, 5)는 1,2,3,4,5 중 하나의 정수를 무작위로 선택\n",
    "\n",
    " - 주로 샘플링, 데이터 증강, 난수 생성 등의 작업에 유용\n",
    "\n",
    " \n",
    "\n",
    "* join(inerable)\n",
    "\n",
    " - join함수는 문자열 메서드로 리스트나 튜플과 같은 반복 가능한 객체(iterable)에 있는 문자열을 하나로 합친다.\n",
    "\n",
    " - join을 호출하는 문자열은 각 요소 사이에 삽입될 구분자 역할을한다.\n",
    "\n",
    " - \" \".join(['I', 'love'])는 \"I love\"로 출력 되듯이 요소를 이어줄 때 요소사이에 값(\" \")을 넣어주는 역할\n",
    "\n",
    " - 리스트나 튜플의 각 요소를 연결하여 문자열로 합칠 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da633b50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "study2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
