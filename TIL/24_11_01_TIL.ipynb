{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "182f63f1",
   "metadata": {},
   "source": [
    "# 내일배움캠프 11/1 TIL\n",
    "\n",
    "오늘은 AI모델활용 강의를 들으며 시작했습니다.\n",
    "\n",
    "오늘의 목표는 4주차까지 듣고 1주차부터 다시 복습을 하여 AI모델 활용에 대한 이해도를 높이는게 목적입니다.\n",
    "\n",
    "강의를 듣는 도중 gensim과 numpy의 호환성문제가 발생했고 uninstall 등 다양한 방법을 사용해보았고 결국 새로운 가상환경을 만들어 해결했습니다.\n",
    "\n",
    "오늘 들은 강의는 코드를 작성하며 실습하는 수업이였습니다.\n",
    "\n",
    "코드를 적으며 기억이 나지 않거나 처음 보는 코드들의 기능을 자세히 알아보며 강의를 들어 이해가 잘 되었습니다.\n",
    "\n",
    " \n",
    "\n",
    "강의를 들으면서 정리한 새롭게 알게된 내용과 까먹은 내용들을 강의 순서에 맞게 정리하겟습니다.\n",
    "\n",
    "그리고 마크다운을 사용해서 TIL을 작성하는게 익숙해지고있으며 이모지를 가져와서 더욱 가독성이 좋게 꾸며야 할 것 같습니다.\n",
    "\n",
    " \n",
    "\n",
    "마지막으로 해결하지 못한 궁금한 점이있습니다.\n",
    "\n",
    " \n",
    "\n",
    "API를 활용해서 모델을 만드는 실습을 했는데 GPT-4o는 내가 알기론 유료 플랜인데 코드를 실행시키면 요금이 청구 되나요? (코드를 실행 시켜보지는 않았습니다 무서버서ㅎㅎ:)\n",
    "\n",
    "\n",
    "\n",
    "----------\n",
    "# AI모델활용 3주차\n",
    "\n",
    "## 3-1 허깅페이스와 트랜스포머\n",
    "\n",
    "### Trnasformers 라이브러리\n",
    "Transformers 라이브러리는 다양한 NLP 모델을 쉽게 사용할 수 있도록 도와주는 Hugging Face의 오픈소스 라이브러리, 이 라이브러리를 통해 최신 NLP 모델들을 불러와 텍스트 생성, 감정 분석, 번역 등 다양한 작업에 활용할 수 있다.\n",
    "\n",
    "> 참고! 모델을 가져와서 사용할 땐 vscode가 좋다!!\n",
    "\n",
    "#### pipeline\n",
    "\n",
    "* ***pipeline이란?***\n",
    " 1. 텍스트 분류: 감정분석, 주제 분류, 등의 작업을 지원하며, 문장의 감정을 예측하거나 특정 카테고리분류 가능\n",
    " 2. 질의 응답: 주어진 문서와 질문을 입력받아 문서에서 질문에 대한 답변을 찾는 작업\n",
    " 3. 텍스트 생성: 주어진 문장을 기반으로 텍스트를 생성하며, 대화생성이나 문장 완성 가능\n",
    " 4. 번역\n",
    " 5. 요약: 긴 텍스트를 간단히 요약해 주요 내용 추출 가능\n",
    " 6. 개체명 인식: 문장 내에서 이름, 위치ㅡ 조직 등을 식별하여 태그를 붙이는 작업\n",
    " 7. 문장 유사도: 두 문장의 유사도를 계산하는 작업으로 의미적 유사성 측정 가능\n",
    "\n",
    "\n",
    "* ***pipeline의 요소 \"text-generation\" 그 외***\n",
    " 1. \"text-generation\": 텍스트 생성 작업 (예: 대화 생성, 글 작성)\n",
    " 2. \"sentiment-analysis\": 간단한 감성어 분석\n",
    " 3. \"question-answering\": 질의응답 작업\n",
    " 4. \"translation_xx_to_yy\": 번역 작업 (예: \"translation_en_to_fr\"는 영어에서 프랑스어로 번역)\n",
    " 5. \"summarization\": 텍스트 요약\n",
    " 6. \"ner\": 개체명 인식 (Named Entity Recognition)\n",
    " 7. \"sentence-similarity\": 문장 유사도 측정\n",
    " 8. \"fill-mask\": 마스크된 텍스트 채우기 (BERT 모델 등에서 사용)\n",
    " 9. \"text-classification\": 텍스트 분류 작업 (예: 감정 분석)\n",
    "\n",
    "#### 임베딩\n",
    "\n",
    "* ***1. 워드 임베딩***\n",
    " - 자연어 처리에서 단어를 벡터로 처리 (워드투백, 페스트 텍스트)\n",
    " \n",
    "* ***2. 센텐스 임베딩***\n",
    " - 문장 전체를 벡터로 처리\n",
    " - 문장간의 유사도 분석, 의미적 관계 분석\n",
    " - 딥러닝, 머신러닝의 입력으로 활용 가능\n",
    " \n",
    "* ***3. 이미지 임베딩***\n",
    " - 이미지 데이터를 벡터로 표현\n",
    " - 이미지의 픽셀 데이터를 저차원 벡터로 분석하고 이미지간의 유사도 분석 가능\n",
    " - 딥러닝의 CNN과 비슷하지만 여러가지 방법을 도입하는게 좋다.\n",
    " \n",
    "* ***4. 그래프 임베딩***\n",
    " - 그래프 구조를 벡터로 표현\n",
    " - 노드간의 관계를 벡터 공간에서 표현\n",
    " - 네트워크 분석, 추천 시스템에서 활용\n",
    " \n",
    "#### 유사도\n",
    "\n",
    "- 두 개의 데이터가 얼마나 비슷한지 수치적으로 표현하는 방법\n",
    "- 임베딩 벡터에 유사도를 측정하는 방법 (***코사인 유사도***, 유클리디안 루컬?)\n",
    "- 벡터간의 거리나 각도를 계산하는 방법이다.\n",
    " \n",
    "------\n",
    "\n",
    "#### 1. 트랜스포머를 활용한 모델사용 pip install transformers==4.37.0\n",
    "\n",
    "* ***result = generator(\"I have a cat\", max_length=100, num_return_sequences=2)***\n",
    " 1. max_length=100는 생성할 문장의 최대 길이를 지정한다.(항상 최대 길이로 문장을 만드는건 아니다.)\n",
    " 2. num_return_sequences=2는 문장의 개수를 지정한다.\n",
    " \n",
    "```python\n",
    "# 1. 트랜스포머를 활용한 모델 사용\n",
    "from transformers import pipeline\n",
    "\n",
    "generator = pipeline(\"text-generation\", model = \"gpt2\")\n",
    "\n",
    "# 텍스트 생성\n",
    "result = generator(\"I have a cat\", max_length=100, num_return_sequences=2)\n",
    "print(result)\n",
    "```\n",
    " \n",
    "------ \n",
    " \n",
    " \n",
    "#### 2. 감정어 분석\n",
    "문장을 해석해서 어느정도의 감정을 캐치하고 값을 출력한다. ex) Positive\n",
    "\n",
    "* ***버트 모델 model=\"roberta-base\":***\n",
    "버트기반 모델은 핵심적인 논리부분은 대규모 데이터셋을 통해 사전학습 시키고 마지막 레이어(번역, 분류 등) 목적에 따라 바꿀 필요가 있는 레이어는 사용자에게 맡기기도 한다.\n",
    "\n",
    "```python\n",
    "# 2. 감성어 분석\n",
    "from transformers import pipeline\n",
    "\n",
    "sentiment_analysis = pipeline(\"sentiment-analysis\", model=\"roberta-base\")\n",
    "result = sentiment_analysis(\"I love you\")\n",
    "print(result)\n",
    "```\n",
    "------------\n",
    "\n",
    "#### 3. 워드 투 백  pip install gensim\n",
    "\n",
    "***from gensim.models import Word2Vec***\n",
    "\n",
    "* 텍스트 데이터에서 단어 간의 의미적 유사성을 벡터로 표현하는 알고리즘\n",
    "\n",
    "***from gensim.utils import simple_preprocess***\n",
    "\n",
    "* 텍스트를 소문자로 변환하고 불필요한 기호를 제거하며 토큰화하여 단어 리스트로 반환\n",
    "\n",
    "\n",
    "***from scipy.spatial.distance import cosine***\n",
    "\n",
    "* 두 벡터 간의 코사인 거리를 계산하며, 유사도를 계산한다.\n",
    "\n",
    "\n",
    "***sentences***\n",
    "\n",
    "* 임베딩을 생성할 문장 리스트\n",
    "\n",
    "***processed = [simple_preprocess(sentence) for sentence in sentences]***\n",
    "\n",
    "* simple_preprocess 함수를 사용하여 각 문장을 토큰화하고 소문자로 변환하여 단어 리스트 반환\n",
    "\n",
    "***Word2Vec(sentences=processed, vector_size=5, window=5, min_count=1, sg=0)***\n",
    "\n",
    "* sentences=processed: 학습에 사용할 문장 리스트\n",
    "\n",
    "* vector_size=5: 각 단어를 5차원 벡터로 임베딩하고. 이 크기는 사용자가 필요에 따라 조정할 수 있다.\n",
    "\n",
    "* window=5: 컨텍스트 윈도우 크기로, 현재 단어와의 최대 거리(단어 수)를 나타낸다.\n",
    "\n",
    "* min_count=1: 최소 등장 빈도로, 1로 설정했으므로 모든 단어가 임베딩에 포함된다.\n",
    "\n",
    "* sg=0: Skip-gram 방식을 사용할지(1), CBOW 방식을 사용할지(0)를 결정하고. 여기서는 CBOW 방식을 사용.\n",
    "\n",
    "***dog = model.wv['dog']와 cat = model.wv['cat']***\n",
    "\n",
    "* dog와 cat이라는 단어의 벡터 표현을 불러온다.\n",
    "\n",
    "***sim = 1 - cosine(dog, cat)***\n",
    "\n",
    "* cosine 함수를 사용하여 dog와 cat 벡터의 코사인 유사도를 계산\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"The quick brown fox jumps over the lazy dog\",\n",
    "    \"I love playing with my pet dog\",\n",
    "    \"The dog barks at the stranger\",\n",
    "    \"The cat sleeps on the sofa\",\n",
    "]\n",
    "\n",
    "processed = [simple_preprocess(sentence) for sentence in sentences]\n",
    "print(processed)\n",
    "\n",
    "model = Word2Vec(sentences=processed, vector_size=5, window=5, min_count=1, sg=0)\n",
    "dog = model.wv['dog']\n",
    "cat = model.wv['cat']\n",
    "\n",
    "sim = 1 - cosine(dog, cat)\n",
    "print(sim)\n",
    "```\n",
    "\n",
    "\n",
    "*****\n",
    "\n",
    "## 3- 2 Hugging Face: 사전 학습 및 파인튜닝 이해하기\n",
    "\n",
    "\n",
    "### 사정학습이란? Per-training\n",
    "- 대규모의 텍스트 데이터셋을 사용해 모델이 일반적인 언어 이해 능력을 학습하는 과정\n",
    "- 단순히  언어의 패턴과 구조를 학습하는 것이 목적\n",
    "\n",
    "#### 특징\n",
    "1. 대규모 데이터셋 이용\n",
    "2. 일반적인 언어 이해 \n",
    "3. 작업 비특화: 특정 작업에 맞춰진 학습이 아닌 전반적인 언어 이해에 초점을 맞춘다.\n",
    "\n",
    "#### 목적\n",
    "사적학습을 통해 모델이 언어의 기본적인 규칙을 배우고 이후 특정 작업에 빠르게 적응하게 만드는 것이다 (Hugging Face의 대부분 모델들은 이 과정을 거쳤다.)\n",
    "\n",
    "* ***Masked Language Modeling (MLM):*** 문장의 일부 단어를 마스킹하고 예측하도록 학습(문맥의 양방향으로 이해 가능)\n",
    "* ***Next Sentence Prediction (NSP):*** 두 문장이 주어졌을 때 두번째 문장이 척 번째 문장 뒤에 자연스럽게 이어지는지 예측한다.(문장간의 관계를 평가)\n",
    "\n",
    "### 파인튜닝이란? Fine-tunung\n",
    "- 사전 학습된 모델을 특정 작업에 맞게 추가로 학습 시키는 과정이다. \n",
    "- Bert를 감정 분석에 사용하려면 사전 학습된 가중치를 유지하면서 감정 분석 작업에 맞게 모델의 가중치를 조정한다.\n",
    "\n",
    "#### 특징\n",
    "1. 작업 특화: 특정 작업(텍스트 분류, 번역, 질의 응답 등)에 맞춰 모델을 최적화하는 과정\n",
    "2. 사전 학습 가중치 활용: 일부 가중치만 조정\n",
    "3. 적은 데이터로도 가능\n",
    "\n",
    "#### 목적\n",
    "- 특정 작업에서 최상의 성능을 발휘하도록 모델을 조정하는 과정이며 사전학습 덕분에 더 빠르고 적은 데이터로 이루어진다.\n",
    "\n",
    "-------------\n",
    "# AI모델 활용 4주차\n",
    "\n",
    "## 4-1 생성형 AI모델 직접 만들기 때 주의!!\n",
    "\n",
    "### 생성형AI란?\n",
    "- 주어진 입력에 따라 새로운 콘텐츠를 생성하는 인공지능 기술\n",
    "- 몇 개의 단어를 입력받아 그에 맞는 문장 생성, 스케치를 바탕으로 이미지 생성 \n",
    "\n",
    "#### 종류\n",
    "1. 텍스트 생성(GPT-3, ChatGPT 등)\n",
    "2. 이미지 생성(DALL-E, Stable Diffusion 등)\n",
    "3. 음악 생성(Magenta 등)\n",
    "\n",
    "#### 어려움\n",
    "1. 대규모 데이터 필요\n",
    "2. 컴퓨터 자원의 한계\n",
    "3. 모델 구조의 복잡성\n",
    "4. 훈련 과정의 불안정성\n",
    "\n",
    "#### 파인튜닝의 필요성!\n",
    "- 파인 튜닝은 사전 학습된 모델을 특정 작업에 맞게 추가로 학습시키는 과정으로, 생성형 AI 모델을 보다 쉽게 적용할 수 있는 방법\n",
    "- 사전 학습된 모델(시간 절감, 높은 성능, 도메인 특화, 작업 맞춤)\n",
    "\n",
    "#### 만들 때 고려할 부분\n",
    "1. 사전학습된 모델 활용\n",
    "2. 클라우드 서비스 활용\n",
    "3. 작은 프로젝트부터 시작하기\n",
    "\n",
    "\n",
    "\n",
    "## 4-2 생성형 AI모델 기본 원리\n",
    "\n",
    "### 랜덤성 Randomness\n",
    "\n",
    "#### 역할\n",
    "- 출력 데이터를 생성할 때 일정한 확률에 따라 다양한 선택지를 고려하게 한다.\n",
    "\n",
    "#### 확률 분포\n",
    "- 학습 데이터를 통해 얻은 확률 분포를 기반으로 새로운 데이터를 생성\n",
    "- 데이터의 분포를 학습해서 새로운 데이터를 생성할 수 있게 되는거다.\n",
    "\n",
    "### 조건성 Conditionality\n",
    "\n",
    "#### 조건 입력\n",
    "- 입력된 조건에 따라 결과를 다르게 생성\n",
    "- 텍스트, 이미지, 오디오 등 다양한 형식의 조건이 있다.\n",
    "\n",
    "#### 중요성\n",
    "- 조건성 덕분에 생성형 모델은 매우 다양한 상황에 적응할 수 있다.(사용자가 원하는 특정 스타일, 주제, 분위기 등)\n",
    "\n",
    "### 결론\n",
    "- 생성형 AI는 랜덤성과 조건성을 결합하여 다양한 결과를 생성한다.\n",
    "- 조건은 출력의 전반적인 틀과 스타일을 결정\n",
    "- 랜덤성은 결과의 세부적인 변화를 만든다.\n",
    "- 두 요소의 상호작용 덕분에 생성형 AI는 창의적이고 예측 불가능한 결과를 생성가능하게 만들어 준다.\n",
    "\n",
    "### ※작동원리\n",
    "\n",
    "#### 텍스트 기반 생성형 모델의 원리\n",
    "1. 입력 토큰화\n",
    "2. 확률 예측\n",
    "3. 랜덤 선택: temperature 파라미터를 조정하여 랜덤성을 조절가능\n",
    "4. 반복 생성\n",
    "\n",
    "#### 이미지 기반 생성형 모델의 원리\n",
    "1. 텍스트 인코딩\n",
    "2. 이미지 생성\n",
    "3. 세부 사항 추가\n",
    "\n",
    "#### 오디오 기반 생성형 모델의 원리\n",
    "1. 텍스트 또는 멜로디 인코딩\n",
    "2. 오디오 생성\n",
    "3. 랜덤성 적용\n",
    "\n",
    "\n",
    "## 4-3 Hugging Face와 Stable Diffusion\n",
    "\n",
    "### 텍스트 생성\n",
    "1. 사전학습된 모델 사용\n",
    "2. 서비스를 활용해서 AIP의 형태로 언어를 형성\n",
    "\n",
    "#### GPT-4o 모델로 텍스트 생성하기\n",
    "\n",
    "> API_KEY는 설정 파일이나 환경변수를 통해 관리해야한다. (코드상에 KEY작성 금지!!)\n",
    "\n",
    "### Stable Diffusion을 활용한 이미지 생성\n",
    "\n",
    "#### Stable Diffusion 모델의 세부 조정\n",
    "- **guidance_scale**: 텍스트 조건을 얼마나 강하게 반영할지 결정하는 파라미터. 값이 높을수록 텍스트 설명에 충실한 이미지를 생성한다.\n",
    "- **num_inference_steps**: 이미지 생성 과정에서의 추론 단계 수를 지정한다. 단계 수가 많을수록 이미지의 품질이 향상되지만, 생성 시간이 길어진다.\n",
    "\n",
    "```python\n",
    "image = pipe(prompt, guidance_scale=7.5, num_inference_steps=50).images[0]\n",
    "```\n",
    "\n",
    "-------------\n",
    "\n",
    "#### 서비스를 활용해서 AIP의 형태로 언어를 형성\n",
    "\n",
    "***from openai import OpenAI***\n",
    "* OpenAI API와 상호작용하는 데 필요한 기능을 제공\n",
    "\n",
    "***client = OpenAI()***\n",
    "* OpenAI 클래스의 인스턴스를 생성하여 client 변수에 할당\n",
    "* client 객체를 통해 OpenAI API의 기능을 사용할 수 있ek.\n",
    "\n",
    "***client.chat.completions.create***\n",
    "* completions.create 메서드를 호출하여 대화형 응답을 생성\n",
    "\n",
    "***role, content***\n",
    "* 역할, 내용\n",
    "\n",
    "```python\n",
    "# 서비스를 활용해서 AIP의 형태로 언어를 형성\n",
    "\n",
    "# JavaScript에 openai환경변수 설정\n",
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"<your OpenAI API key>\"\n",
    "\n",
    "# python\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "    # 필요되는 정보들\n",
    "  model=\"gpt-4o\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"너는 환영 인사를 하는 인공지능이야, 농담을 넣어 재미있게해줘\"},\n",
    "    {\"role\": \"user\", \"content\": \"안녕?\"}  \n",
    "  ]\n",
    ")\n",
    "\n",
    "print(\"답변: \" + completion.choices[0].message.content)\n",
    "```\n",
    "---------\n",
    "\n",
    "***from diffusers import StableDiffusionPipeline***\n",
    "* 텍스트 설명을 기반으로 이미지를 생성하는 파이프라인을 제공\n",
    "\n",
    "***pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\"***\n",
    "* \"CompVis/stable-diffusion-v1-4\"는 모델의 이름이며, Hugging Face 허브에서 가져올 모델 경로\n",
    "* torch_dtype=torch.float16은 16비트 부동 소수점 데이터를 사용하도록 설정\n",
    "\n",
    "***pipe = pipe.to(\"cuda\")***\n",
    "* pipe 객체를 GPU로 옮긴다. \"cuda\"는 GPU 장치를 의미하며, 이를 통해 모델의 연산이 GPU에서 수행되어 속도를 높일 수 있다.\n",
    "\n",
    "***prompt = \"A futuristic cityscape with flying cars at sunset\"***\n",
    "* 생성할 이미지의 텍스트 설명을 정의\n",
    "\n",
    "***image = pipe(prompt).images[0]***\n",
    "* 텍스트 설명을 기반으로 이미지 생성 요청을 처리\n",
    "* images[0]은 생성된 첫 번째 이미지를 선택한다.\n",
    "\n",
    "***image.save(\"generated_image.png\")***\n",
    "* \"generated_image.png\"라는 파일명으로 저장\n",
    "\n",
    "```python\n",
    "# stable diffusion 모델 예제 코드\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import torch\n",
    "\n",
    "# Stable Diffusion 파이프라인 로드\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")  # GPU 사용\n",
    "\n",
    "# 텍스트 설명을 기반으로 이미지 생성\n",
    "prompt = \"A futuristic cityscape with flying cars at sunset\"\n",
    "image = pipe(prompt).images[0]\n",
    "\n",
    "# 생성된 이미지 저장 및 출력\n",
    "image.save(\"generated_image.png\")\n",
    "image.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1215f1a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "study2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
