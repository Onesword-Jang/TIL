{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d159d8d5",
   "metadata": {},
   "source": [
    "# AI모델활용 1주차\n",
    "\n",
    "## 1-1 API란?\n",
    "* 프로그램끼리 통신하는 인터페이스이다.\n",
    "* 예시: 구글(비젼API), 챗GPT, 일레븐 랩스\n",
    "\n",
    "### 사전학습 모델: 많은 데이터로 미리 학습된 AI모델\n",
    "* 학습과정을 생략하고 바로 예측이나 분류가 가능\n",
    "* 모델 단위로 제공 되어서 모델결합에 좋다.\n",
    "* 안정성이 좋다.\n",
    "\n",
    "### AI에 대한 개념을 알아야 한다!\n",
    "* AI가 어떤 방식으로 작동하는지 알면 문제해결에 필요한 도구나 모델을 선택하는데 좋다.\n",
    "* AI의 한계를 인지하고 인식해야지 현실적인 기대를 설정하고 행동할 수 있다.\n",
    "* AI가 제공하는 결과를 이해하고 의미를 파악할 수 있어야 잘 활용할 수 있다.\n",
    "\n",
    "\n",
    "## 1-2 패키지 관리\n",
    "\n",
    "### 패키지란?\n",
    "패키지는 여러 모듈들을 묶어놓은 하나의 디렉토리입니다. 파이썬에서는 다양한 기능을 제공하는 패키지들이 있어서, 개발자가 직접 모든 기능을 구현하지 않아도 됩니다.\n",
    "ex) pandas, numpym, pytorch 등\n",
    "> 주의! 같은 패키지라도 버젼에 따라 기능이 다르다!!\n",
    "\n",
    "* pip이란? 파이썬 패키지 관리자\n",
    "* pip list로 본인의 환경에 설치된 패키지들을 보여준다\n",
    "* 가상환경이란? 프로젝트마다 독립된 환경을 구성해주는 도구이며, 다른 프로젝트와 패키지 간 충돌을 방지를 한다.\n",
    "* which python으로 내 파이썬이 어디에서 시작하는지 볼 수 있다.\n",
    "\n",
    "## 1-3 허깅페이스\n",
    "\n",
    "### 허깅페이스란\n",
    "자연어 처리 중심으로 다양한 API모델들을 제공하는 플랫폼\n",
    "\n",
    "#### 특징\n",
    "1. transformers 라이브러리\n",
    "이 라이브러리는 BERT, GPT-3 같은 최신 NLP 모델을 쉽게 사용할 수 있다.\n",
    "2. 모델허브\n",
    "수천 개의 미리 학습된 모델들이 모여있는 곳이며,클릭 몇 번으로 모델을 가져다 쓸 수 있다.\n",
    "3. 커뮤니티 중심\n",
    "\n",
    "#### 장점\n",
    "1. 쉬운 접근성\n",
    "2. 광범위한 모델 선택\n",
    "3. 오픈소스\n",
    "4. 커뮤니케이션 지원\n",
    "\n",
    "#### 단점\n",
    "1. 리소스 요구량이 큰 모델들이있다.\n",
    "2. 복잡한 초기 설정\n",
    "3. 특화된 모델\n",
    "\n",
    "경고창 무시\n",
    "import warnings\n",
    "warnings.filterwarnigs('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e5392b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\USER\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Hello, it's been a while since I've posted a post about this. I'm not sure if I've been able to get it to work, but I'm sure it's something that I'll be able to do.\\n\\nI'm\""
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장의 앞부분을 전달하면 뒷 부분을 자동으로 만들어주는 인공지능 서비스\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# 토크나이저와 모델 만들기\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# 입력에 사용될 문장\n",
    "text = \"Hello, it's\"\n",
    "\n",
    "# 인코딩\n",
    "encoded_input = tokenizer(text, return_tensors = 'pt')\n",
    "\n",
    "# 모델의 결과 값을 저장할 output\n",
    "output = model.generate(encoded_input['input_ids'], max_length = 50)\n",
    "generated_text = tokenizer.decode(output[0], skip_special_tokens = True) # 사람이 인지가능하게 output을 텍스트로 바꾸는 작업\n",
    "\n",
    "generated_text #제네레이트 출력"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcaab9b",
   "metadata": {},
   "source": [
    "# AI모델 활용 2주차\n",
    "\n",
    "## 2-2 API로 인공지능 활용하기\n",
    "API는 Application Programming Interface의 줄임말로, 프로그램 간에 데이터를 주고받을 수 있게 해주는 인터페이스\n",
    "\n",
    "* 텍스트 생성 API: ChatGPT\n",
    "* 음성 합성 API: ElevenLabs\n",
    "* 컴퓨터 비젼 API: Google Vision AI\n",
    "* 음성인식 API: Google Cloud Speech-to-Text\n",
    "* 번역 API: DeepL\n",
    "\n",
    "### 장점\n",
    "1. 손쉬운 사용\n",
    "2. 신속한 개발\n",
    "3. 확장성\n",
    "\n",
    "### 단점\n",
    "1. 비용: 사용에 따른 비용 발생\n",
    "2. 제한된 언어: 커스터마이징의 제한\n",
    "3. 의존성: 해당 서비스가 중단되거나 변경될 때\n",
    "\n",
    "### 팁!\n",
    "1. 문서읽기: 사용방법이 문서에 포함되어있어 필수!\n",
    "2. API 키 관리\n",
    "3. 무료 할당량 체크\n",
    "\n",
    "### 강의 8분 쯤 부터 API소개를 다시 듣고 필기하자.\n",
    "\n",
    "## 2-3 일단 만들어보자 PyTorch, Transformer\n",
    "\n",
    "### 시작하기 전 pip한 모듈들\n",
    "1. pip install transformers\n",
    "2. pip install sentencepiece sacremoses\n",
    "3. pip install importlib_metadata\n",
    "\n",
    "### 모델 불러오기\n",
    "* d_model 단어의 임베딩 차원수를 나타낸다\n",
    "* nhead는 멀티헤드레코드에 헤드수\n",
    " * **멀티헤드레코드란?**\n",
    " 데이터베이스 구조에서 하나의 테이블에 여러 종류의 레코드를 저장할 수 있게 성계된 구조를 의미하며, 8개의 주의 헤드가 생기고 각 헤드가 독립적으로 주의계산을 수행한 다음 모든 헤드의 결과가 결합된다.\n",
    "* layers는 어느정도의 레이어로 형성할 것인지\n",
    "\n",
    "### 모델 학습 준비\n",
    "* 파라미터가 많고 깊고 복잡한 모델일 경우 lr(러닝레이트)를 작게 설정하는 경우가 많음\n",
    "\n",
    "### 모델 학습 시키기\n",
    "* epoch는 모델을 학습하는 하나의 주기를 의미\n",
    "* num_epochs는 총 학습 주기 수를 의미\n",
    "* optimizer.zero_grad()는 매 에포크의 학습 시작 전에 이전 에포크에서 누적된 기울기 초기화\n",
    "* output = model(src, tgt)\n",
    " * src는 소스 입력 데이터를 나타내며, 트랜스포머 모델의 입력으로 들어가는 원본 텍스트나 시퀀스\n",
    " * tgt는 대상 데이터(target data)로, 일반적으로 모델이 예측해야 하는 정답 시퀀스\n",
    "* criterion은 손실 함수로, 예측된 출력(output)과 실제 레이블(tgt_labels) 사이의 오차를 계산 \n",
    "* loss.backward()는 손실 값에 대한 가중치의 기울기를 계산하는 과정으로, 역전파(backpropagation)라고도 한다.\n",
    "* optimizer.step()은 계산된 기울기를 바탕으로 모델의 가중치를 업데이트하는 단계\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e02fefbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Transformer\n",
    "\n",
    "# transformer 모델 불러오기\n",
    "model = Transformer(d_model=512, nhead=8, num_encoder_layers=6, num_decoder_layers=6)\n",
    "\n",
    "# 모델학습 준비\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 모델학습 시키기\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(src, tgt)\n",
    "    loss = criterion(output, tgt_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e94e396",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\USER/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
      "Using cache found in C:\\Users\\USER/.cache\\torch\\hub\\huggingface_pytorch-transformers_main\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Once upon a time, the world was a place of great beauty and great danger. The world was'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 사전 학습된 모델 활용 예시\n",
    "model = torch.hub.load('huggingface/pytorch-transformers', 'modelForCausalLM', 'gpt2')\n",
    "tokenizer = torch.hub.load('huggingface/pytorch-transformers', 'tokenizer', 'gpt2')\n",
    "\n",
    "# 예제\n",
    "input_text = \"Once upon a time\"\n",
    "\n",
    "# 인풋 토큰화\n",
    "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "\n",
    "# 토큰화된 인풋을 통해 아웃풋 생성\n",
    "output = model.generate(input_ids, max_length=20, num_return_sequences=1)\n",
    "\n",
    "# 아웃풋 토큰화\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d9f201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "study2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
